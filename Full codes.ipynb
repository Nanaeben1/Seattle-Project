{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ffc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of CSV file names\n",
    "csv_files = [\n",
    "    'Electrical_permits_part_1.csv',\n",
    "    'Electrical_permits_part_2.csv',\n",
    "    'Electrical_permits_part_3.csv',\n",
    "    'Electrical_permits_part_4.csv',\n",
    "    'Electrical_permits_part_5.csv',\n",
    "    'Electrical_permits_part_6.csv',\n",
    "    'Electrical_permits_part_7.csv',\n",
    "    'Electrical_permits_part_8.csv'\n",
    "]\n",
    "\n",
    "# Read and concatenate all the CSV files into one DataFrame\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('Electrical_permits.csv', index=False)\n",
    "\n",
    "# Optionally, display the first few rows of the merged DataFrame\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b448c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Electrical_permits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9014b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('Electrical_permits.csv')\n",
    "\n",
    "# Strip any leading or trailing spaces in column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Print the column names to ensure they are correct\n",
    "print(\"Original columns:\", df.columns)\n",
    "\n",
    "# Check for the presence of the predictor columns\n",
    "if 'PermitClassMapped' not in df.columns or 'PermitTypeMapped' not in df.columns or 'PermitClass' not in df.columns:\n",
    "    raise KeyError(\"The columns 'PermitClass', 'PermitClassMapped', or 'PermitTypeMapped' are not found in the DataFrame\")\n",
    "\n",
    "# Extract rows with and without missing values\n",
    "non_missing_data = df.dropna(subset=['EstProjectCost'])\n",
    "missing_data = df[df['EstProjectCost'].isnull()]\n",
    "\n",
    "# Convert categorical variables to numerical using get_dummies\n",
    "non_missing_data = pd.get_dummies(non_missing_data, columns=['PermitClass', 'PermitClassMapped', 'PermitTypeMapped'])\n",
    "missing_data = pd.get_dummies(missing_data, columns=['PermitClass', 'PermitClassMapped', 'PermitTypeMapped'])\n",
    "\n",
    "# Ensure both datasets have the same dummy variable columns\n",
    "missing_data = missing_data.reindex(columns=non_missing_data.columns, fill_value=0)\n",
    "\n",
    "# Print the columns after get_dummies to ensure they match\n",
    "print(\"Non-missing data columns after get_dummies:\", non_missing_data.columns)\n",
    "print(\"Missing data columns after reindex:\", missing_data.columns)\n",
    "\n",
    "# Choose predictors - update this list based on the actual dummy columns created\n",
    "predictors = [col for col in non_missing_data.columns if col.startswith('PermitClass_') or col.startswith('PermitClassMapped_') or col.startswith('PermitTypeMapped_')]\n",
    "\n",
    "# Train the regression model\n",
    "model = LinearRegression()\n",
    "model.fit(non_missing_data[predictors], non_missing_data['EstProjectCost'])\n",
    "\n",
    "# Predict the missing values\n",
    "predicted_values = model.predict(missing_data[predictors])\n",
    "\n",
    "# Fill in the missing values with the predicted values\n",
    "df.loc[df['EstProjectCost'].isnull(), 'EstProjectCost'] = predicted_values\n",
    "\n",
    "# Verify if the missing values are filled\n",
    "print(df['EstProjectCost'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f54415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('Electrical_permits.csv')\n",
    "\n",
    "# Strip any leading or trailing spaces in column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Print the column names to ensure they are correct\n",
    "print(\"Original columns:\", df.columns)\n",
    "\n",
    "# Check for the presence of the predictor columns\n",
    "if 'PermitClassMapped' not in df.columns or 'PermitTypeMapped' not in df.columns or 'PermitClass' not in df.columns:\n",
    "    raise KeyError(\"The columns 'PermitClass', 'PermitClassMapped', or 'PermitTypeMapped' are not found in the DataFrame\")\n",
    "\n",
    "# Extract rows with and without missing values in EstProjectCost\n",
    "non_missing_data = df.dropna(subset=['EstProjectCost'])\n",
    "missing_data = df[df['EstProjectCost'].isnull()]\n",
    "\n",
    "# Convert categorical variables to numerical using get_dummies\n",
    "non_missing_data = pd.get_dummies(non_missing_data, columns=['PermitClass', 'PermitClassMapped', 'PermitTypeMapped'])\n",
    "missing_data = pd.get_dummies(missing_data, columns=['PermitClass', 'PermitClassMapped', 'PermitTypeMapped'])\n",
    "\n",
    "# Ensure both datasets have the same dummy variable columns\n",
    "missing_data = missing_data.reindex(columns=non_missing_data.columns, fill_value=0)\n",
    "\n",
    "# Print the columns after get_dummies to ensure they match\n",
    "print(\"Non-missing data columns after get_dummies:\", non_missing_data.columns)\n",
    "print(\"Missing data columns after reindex:\", missing_data.columns)\n",
    "\n",
    "# Choose predictors - update this list based on the actual dummy columns created\n",
    "predictors = [col for col in non_missing_data.columns if col.startswith('PermitClass_') or col.startswith('PermitClassMapped_') or col.startswith('PermitTypeMapped_')]\n",
    "\n",
    "# Train the regression model\n",
    "model = LinearRegression()\n",
    "model.fit(non_missing_data[predictors], non_missing_data['EstProjectCost'])\n",
    "\n",
    "# Predict the missing values\n",
    "predicted_values = model.predict(missing_data[predictors])\n",
    "\n",
    "# Fill in the missing values with the predicted values\n",
    "df.loc[df['EstProjectCost'].isnull(), 'EstProjectCost'] = predicted_values\n",
    "\n",
    "# Fill other missing values using backfill or forward fill\n",
    "df.fillna(method='bfill', inplace=True)\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Define thresholds\n",
    "single_family_threshold = 15000000  # 15 million\n",
    "multi_family_threshold = 50000000   # 50 million\n",
    "\n",
    "# Check unique values in 'PermitClass' to ensure categories are correct\n",
    "print(\"Unique values in 'PermitClass':\", df['PermitClass'].unique())\n",
    "\n",
    "# Print descriptive statistics for 'EstProjectCost'\n",
    "print(\"Descriptive statistics for 'EstProjectCost':\")\n",
    "print(df['EstProjectCost'].describe())\n",
    "\n",
    "# Print the number of rows before filtering\n",
    "print(\"Number of rows before filtering:\", len(df))\n",
    "\n",
    "# Filter rows based on the conditions\n",
    "filtered_df = df[\n",
    "    ~(\n",
    "        ((df['PermitClass'] == 'Single Family/Duplex') & (df['EstProjectCost'] > single_family_threshold)) |\n",
    "        ((df['PermitClass'] == 'MultiFamily') & (df['EstProjectCost'] > multi_family_threshold))\n",
    "    )\n",
    "]\n",
    "\n",
    "# Print the number of rows after filtering\n",
    "print(\"Number of rows after filtering:\", len(filtered_df))\n",
    "\n",
    "# Verify if the missing values are filled\n",
    "print(filtered_df.isnull().sum())\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file\n",
    "filtered_df.to_csv('Cleaned_Electrical_permits55.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfb93c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc9cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert date columns to datetime with error handling\n",
    "date_columns = ['AppliedDate', 'IssuedDate', 'ExpiresDate', 'CompletedDate']\n",
    "for col in date_columns:\n",
    "    filtered_df[col] = pd.to_datetime(filtered_df[col], errors='coerce')\n",
    "\n",
    "# Verify if the datetime conversion was successful\n",
    "print(filtered_df[date_columns].dtypes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531bd3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['ApplicationToIssueTime'] = (filtered_df['IssuedDate'] - filtered_df['AppliedDate']).dt.days\n",
    "filtered_df['IssueToCompletionTime'] = (filtered_df['CompletedDate'] - filtered_df['IssuedDate']).dt.days\n",
    "filtered_df['TotalPermitTime'] = (filtered_df['CompletedDate'] - filtered_df['AppliedDate']).dt.days\n",
    "filtered_df['AppliedYear'] = filtered_df['AppliedDate'].dt.year\n",
    "filtered_df['AppliedMonth'] = filtered_df['AppliedDate'].dt.month\n",
    "filtered_df['AppliedDay'] = filtered_df['AppliedDate'].dt.day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c806ced4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc67761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e7ee7fb",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c133a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e48ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of Permits by Year and Month\n",
    "filtered_df['Year'] = filtered_df['AppliedDate'].dt.year\n",
    "filtered_df['Month'] = filtered_df['AppliedDate'].dt.month\n",
    "heatmap_data = filtered_df.pivot_table(index='Year', columns='Month', aggfunc='size', fill_value=0)\n",
    "fig = px.imshow(heatmap_data, labels={'x': 'Month', 'y': 'Year', 'color': 'Number of Permits'}, title='Heatmap of Permits by Year and Month')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0dc06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot of Estimated Project Costs by Permit Class\n",
    "fig = px.box(filtered_df, x='PermitClass', y='EstProjectCost', labels={'PermitClass': 'Permit Class', 'EstProjectCost': 'Estimated Project Cost'}, title='Box Plot of Estimated Project Costs by Permit Class')\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086627ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure only numeric columns are used for correlation matrix\n",
    "numeric_df = filtered_df.select_dtypes(include=[np.number])\n",
    "\n",
    "\n",
    "# Correlation Matrix Heatmap\n",
    "correlation_matrix = numeric_df.corr()\n",
    "fig = ff.create_annotated_heatmap(z=correlation_matrix.values, x=list(correlation_matrix.columns), y=list(correlation_matrix.index), annotation_text=correlation_matrix.round(2).values, showscale=True, colorscale='Viridis')\n",
    "fig.update_layout(title='Correlation Matrix Heatmap')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121468cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the number of permits issued over time\n",
    "plt.figure(figsize=(14, 7))\n",
    "df['AppliedDate'].groupby(filtered_df['AppliedDate'].dt.to_period('M')).count().plot(kind='line')\n",
    "plt.title('Number of Permits Applied Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Permits')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bf6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Feature Engineering\n",
    "df['Month'] = df['AppliedDate'].dt.month\n",
    "df['Year'] = df['AppliedDate'].dt.year\n",
    "df['DayOfWeek'] = df['AppliedDate'].dt.dayofweek\n",
    "df['Quarter'] = df['AppliedDate'].dt.quarter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658950d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data by contractor\n",
    "contractor_performance = filtered_df.groupby('ContractorCompanyName').agg({\n",
    "    'EstProjectCost': ['mean', 'median', 'sum'],\n",
    "    'ApplicationToIssueTime': 'mean',\n",
    "    'IssueToCompletionTime': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the MultiIndex columns\n",
    "contractor_performance.columns = ['ContractorCompanyName', 'AvgProjectCost', 'MedianProjectCost', 'TotalProjectCost', 'AvgApplicationToIssueTime', 'AvgIssueToCompletionTime']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad87dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 contractors by average project cost\n",
    "top_contractors = contractor_performance.sort_values(by='AvgProjectCost', ascending=False).head(10)\n",
    "\n",
    "# Visualize the top contractors\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.barh(top_contractors['ContractorCompanyName'], top_contractors['AvgProjectCost'])\n",
    "plt.xlabel('Average Project Cost')\n",
    "plt.ylabel('Contractor Company Name')\n",
    "plt.title('Top 10 Contractors by Average Project Cost')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca22dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7fd9e9f",
   "metadata": {},
   "source": [
    "# Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a52c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Decompose the time series\n",
    "decomposition = seasonal_decompose(monthly_permits, model='additive')\n",
    "\n",
    "# Plot the decomposed components\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.plot(decomposition.observed, label='Observed')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(412)\n",
    "plt.plot(decomposition.trend, label='Trend')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(413)\n",
    "plt.plot(decomposition.seasonal, label='Seasonal')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(414)\n",
    "plt.plot(decomposition.resid, label='Residual')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba0772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the decomposed components\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "# Extract seasonal patterns\n",
    "seasonal_pattern = seasonal.groupby(seasonal.index.month).mean()\n",
    "\n",
    "# Plot the seasonal pattern\n",
    "plt.figure(figsize=(14, 7))\n",
    "seasonal_pattern.plot(marker='o')\n",
    "plt.title('Average Seasonal Pattern')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Seasonal Effect')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Recommendations\n",
    "# - Identify peak months for permit issuance\n",
    "peak_months = seasonal_pattern.nlargest(3)\n",
    "low_months = seasonal_pattern.nsmallest(3)\n",
    "\n",
    "print(\"Peak Months for Permit Issuance:\")\n",
    "print(peak_months)\n",
    "\n",
    "print(\"\\nLow Months for Permit Issuance:\")\n",
    "print(low_months)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a75d700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "096d6479",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c331d2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Assuming `filtered_df` is your DataFrame and already has the necessary features\n",
    "features = [\n",
    "    'ApplicationToIssueTime', 'IssueToCompletionTime', 'TotalPermitTime', \n",
    "    'AppliedYear', 'AppliedMonth', 'AppliedDay'\n",
    "]\n",
    "\n",
    "X = filtered_df[features]\n",
    "\n",
    "# Define the target variable (if predicting time-based data, modify accordingly)\n",
    "# For this example, we need to set a target variable. Let's assume you're predicting `TotalPermitTime`\n",
    "y = filtered_df['TotalPermitTime']\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the XGBoost model\n",
    "model_xgb = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=6)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mse_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f'XGBoost MAE: {mae_xgb}')\n",
    "print(f'XGBoost MSE: {mse_xgb}')\n",
    "print(f'XGBoost RMSE: {rmse_xgb}')\n",
    "print(f'XGBoost R-squared: {r2_xgb}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb271aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "\n",
    "\n",
    "# For XGBoost\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot actual vs. predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# XGBoost predictions\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(y_test.values, label='Actual Values', color='blue')\n",
    "plt.plot(y_pred_xgb, label='XGBoost Predictions', color='red', linestyle='--')\n",
    "plt.title('XGBoost Model Predictions vs Actual Values')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Target Value')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c4278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_mae = cross_val_score(model_xgb, X_scaled, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "cv_mse = cross_val_score(model_xgb, X_scaled, y, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_r2 = cross_val_score(model_xgb, X_scaled, y, cv=5, scoring='r2')\n",
    "\n",
    "print(f'Cross-Validated MAE: {-cv_mae.mean()}')\n",
    "print(f'Cross-Validated MSE: {-cv_mse.mean()}')\n",
    "print(f'Cross-Validated R-squared: {cv_r2.mean()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea5422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "residuals = y_test - y_pred_xgb\n",
    "\n",
    "# Plotting residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Residuals vs. Predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred_xgb, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Residuals vs. Predicted Values')\n",
    "plt.xlabel('Predicted Value')\n",
    "plt.ylabel('Residual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61727366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    model_xgb, X_scaled, y, cv=5, scoring='neg_mean_squared_error', \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "train_scores_mean = -train_scores.mean(axis=1)\n",
    "test_scores_mean = -test_scores.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training error\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Validation error\")\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Training examples')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671731f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model_xgb.feature_importances_\n",
    "feature_names = filtered_df[features].columns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, importances)\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
